[AUDIO LOGO]
Welcome back. Last time, we spoke about various methods of decoding. In this lesson, we'll talk about hallucination. So what is hallucination? Hallucination has been defined a number of slightly different ways. But for the purpose of our discussion, let's define hallucination to be text, generated by a model, that is not grounded by any data the model has been exposed to.
In other words, this is generated text that is unsupported by any of the data the model has been trained on or any of the data that is fed into it as input. At times statements that are nonsensical or factually incorrect are also considered to be hallucinations. For example, consider the text on the slide, which contains a bolded statement that is not factually correct.
Typically, statements like these are considered to be hallucinations. Specifically, this text states that in the United States, people gradually adopted the practice of driving on the left side of the road. This is factually incorrect and would typically be considered a hallucination.
One thing to note about this example is that the generated text is completely fluent and even starts off correct. Many people might believe that model hallucinations are obvious and easy to identify, like the one on this slide. But a serious issue with hallucination is that, oftentimes, hallucinations are subtle.
For example, the model might produce a hallucination by adding a single adjective incorrectly to a noun phrase. For example, outputting something like Barack Obama was the first president of the United States, which is wrong. This is especially problematic and dangerous when the model is generating text about a topic that the consumer does not know much about and cannot verify the veracity of easily.
As you might imagine, the threat of hallucination is one of the biggest challenges to safely deploying LLMs. As a friend of mine, Samir Singh, a professor at UC Irvine said, "Think about LLMs as chameleons. They're trying to generate text that blends in with human generated text, whether or not it's true. It just needs to sound true."
Additionally, I've heard it said, and I don't to whom this idea is attributed, that all text generated from an LLM is hallucinated. The generations just happen to be correct most of the time. All this is to say that LLM-generated text is somewhat unpredictable. It's often good, fluent, and accurate, but sometimes, it's not factual or even unsafe.
It can contain blatant or subtle non-factual statements which can be difficult to reliably spot. One important thing to note is that there is no known method that will eliminate hallucination, with 100% certainty. On the other hand, there is a growing set of best practices and typical precautions to take when using LLMs to generate text.
As one example, there is some evidence that shows that retrieval-augmented systems hallucinate less than zero-shot LLMs. In a related line of work that is growing in popularity, researchers are developing methods for measuring the groundedness of LLM-generated output.
These methods work by taking a sentence generated by an LLM and a candidate's supporting document and outputting whether the document supports the output sentence or not. These methods work by training a separate model to perform natural language inference or NLI, which is the task that's been studied in the NLP community for a long time.
In this task, you're given a premise that is some text and a hypothesis, maybe a generated sentence. And the goal is to predict, whether the premise entails the hypothesis. That is the supporting text implies the generated sentence. There exists some off the shelf models that perform this kind of prediction, one of which is called true.
And they work relatively well. However, they tend to be pretty conservative. At the same time, new grounded versions of question answering have been proposed, which is the task of answering questions while also citing the sources of the answer being provided. And more work has focused on citation and attribution of LLM-generated content.
In this way, we can see that the research community thinks of hallucination as a serious problem and, as such, is devoting significant resources to study the phenomenon and trying to figure out ways to mitigate or avoid it. This concludes our discussion on hallucination. In our final lesson, we'll discuss applications of large language models.
