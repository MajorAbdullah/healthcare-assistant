[AUDIO LOGO]

Hello and welcome to this lesson on OCI generative AI service. OCI generative AI service is a fully managed service that provides a set of customizable large language models available via single API to build generative AI applications. And what I mean by single API access is that you have the flexibility to use different foundational models with minimal code changes.

The service is also serverless, meaning you don't have to manage any infrastructure. There are three key characteristics of the service. The first being the service provides a choice of pre-trained foundational models from Meta and Cohere. The second is the service enables flexible fine-tuning. And you can create custom models by fine-tuning pre-trained foundational models with your own data set. And last, the service enables dedicated AI clusters. These are GPU-based compute resources that host your fine-tuning and inference workloads, and we'll go through each of them in a little bit more detail.

So before we do that, how does the generative AI service really work? Well, it's you provide text input as a prompt and you get a response. You can ask questions in natural language and optionally submit text such as documents, email, product reviews, et cetera, to the generative AI service, and it reasons over the text and provides intelligent answers. So it is built to understand, generate, and process human language at a massive scale.

And some of the use cases that it unlocks are things like chat, so you could have dialogue, you could generate text. It could be used for information retrieval or even things like semantic search. We'll look into each of these in more detail.

So the first major characteristics of the service is these pre-trained foundational models. And there are two kind of pre-trained foundational models, chat models and embedding models. So for the chat model, we have the following models, which are shown on the slide. So command-r-plus, command-r-16k model, and llama 3-70billion-instruct model.

Now command-r and command-r chat models. Both belong to Cohere family of LLMs, but they differ in their capabilities, use cases, and pricing. R-plus is more powerful and can handle larger number of requests but is more expensive to use. So for example, for r-plus, the user prompt can be up to 128,000 tokens, while for command-r, it is limited to 16,000 tokens. So depending on what your use case might be, you might choose either command-r for more advanced use cases or-- command-r-plus, or you would choose command-r for more like entry level use cases, but it's more affordable.

The thing is, these models, you can ask questions to these model and get conversational responses. What I mean by that is the chat models keep the context of your previous prompts, and you can continue the chat with follow-up questions. These models are also instruction following models or instruction-tuned models, where we take the base models and run them through additional training called instruction tuning. These allows the model to better follow human language instructions, such as generate an email or summarize this text.

The second class of pre-trained foundational models are the embedding models. And here we have the embed English model and the embed multilingual model. And before, I describe them in a little bit more detail. What we mean by embeddings? Embeddings are nothing but text converted to vector of numbers. Embeddings make it easy for computers to understand the relationship between pieces of text.

Embeddings are mostly used for areas such as semantic search, where the search function focuses on the meaning of the text that it's searching rather than finding results based on keywords, what is typically referred to as lexical search. So that's where you would use these embedding models. Now we also support multilingual model. And the multilingual model supports 100-plus languages and can be used to search within a language. So for example, you can search with a French query on a set of French documents or across languages, search with a Chinese query on French documents. So that is also enabled by these multilingual model, by this multilingual model. So those are the two classes of pre-trained foundational models, chat models and embedding models.

The service also enables fine-tuning, and fine-tuning is nothing but optimizing a pre-trained foundational model on a smaller domain specific data set. So you can see, we take a pre-trained model, provide custom data, and then we end up with a custom model. And the process is called fine-tuning. There are two main benefits, fine-tuning improves model performance on specific domains, specific tasks, and it also improves model efficiency.

You use fine-tuning when a pre-trained model doesn't perform your task well or you want to teach it something new. And the service, OCI generative AI service, enables something called t-few fine-tuning, which enables fast and efficient customization. In this case, what we do is the fine-tuning method inserts-- this t-few fine-tuning method inserts new layers in the base model and selectively updates only a fraction of the models weight. Doing so, it reduces the overall training time and the cost compared to doing something like a vanilla fine-tuning, where you update all the layers or most of the layers in the base model.

And finally, last but not the least, the service provides dedicated AI clusters. And these are basically GPU-based compute resources that host the customers fine-tuning and inference workloads. What the service does is it establishes AI dedicated clusters which includes dedicated GPU, as you can see in the GPU pool here, and an exclusive RDMA cluster networking for connecting these GPUs. And these RDMA cluster networking allows you to create large clusters of GPU instances with ultra low latency networking.

And the thing to keep in mind, the GPUs allocated for a customer's task, gen AI task are isolated from other GPUs. So we keep that security mechanism in place. So that's pretty much it. OCI Generative AI service, the three main characteristics are pre-trained foundational models, the flexible fine-tuning, and dedicated clusters. I hope you found this lesson useful. Thanks for your time.
