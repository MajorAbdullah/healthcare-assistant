Welcome to this lesson on fine-tuning and inference in OCI Generative AI service. Before we go dive deeper, let us first look at what fine-tuning and inference really mean. We have covered this a couple of times already in this module. Fine-autotuning is basically taking a pre-trained foundational model, and providing additional training using custom data, as is being shown here.

In traditional machine learning terminology, inference refers to the process of using a trained machine learning model to make predictions or decisions based on new input data. But in case of large language models, what inference refers to is the model receiving new text as input, and generating output text based on what it has learned during training and fine-tuning. So this is basically how inference and fine-tuning look like in context of large language models.

Now, let us look at fine-tuning workflow in the OCI Generative AI service. Before we go and look at the workflow itself, there is one term you need to know, which is custom model. Now, custom model is a model that you create by using a pre-trained model as a base, and using your own data set to fine tune that model. It's called a custom model.

So the process is quite straightforward. The first step is you create a dedicated AI cluster. And the type of the cluster is fine-tuning cluster. Then you gather training data. And you could actually start here. You could gather training data, and then create a dedicated AI cluster. So you could interchange these steps.

The third step is to kick start the fine-tuning process. And the fourth step is where you get the fine-tuned or the custom model, is created, and you have that model. So this is what the workflow looks like. In a subsequent demo, we will actually walk through this process in the OCI Console.

Now, similar to that, let us look at the inference workflow in the OCI Generative AI service. Now, before we go, just a quick term here, which is called Model Endpoint. Model Endpoint is a designated point on the dedicated AI cluster, where the large language model can accept user request, and send back responses, such as models generated text. So that's the endpoint where you can receive, request, and send response.

Quite straightforward process. The first step is, you create a dedicated AI cluster, like we did with fine-tuning. Now the cluster is called a hosting cluster. And then you create the endpoint. And then, basically, you serve the traffic, the production load, or you serve the model. So this is what the process looks like.

Now, before we go and talk about fine-tuning itself, let us quickly look at the dedicated AI clusters. We have covered this in the intro lesson. The main idea with dedicated AI cluster is, the service is giving you a single tenant GPU deployment, where the GPUs in the cluster only host your custom models. Because the endpoint is not shared with other customers, the model throughput is consistent. And because the model throughput is consistent, it's easy to determine the cluster size, what kind of cluster you need to run your models. And we have a lesson where we are going to look at dedicated AI clusters, sizing, and pricing later on.

Now, there are two kinds of clusters which you can create. The first cluster type is the fine-tuning cluster, as the name signifies. This is the cluster which is used for training, a pre-trained foundational model. And the second cluster type is the hosting cluster. This is where you host your custom model endpoints for inference. So for training, fine-tuning, you use the fine-tuning cluster. And for inference, you use the hosting cluster type.

So with OCI Generative AI service, there is a fine-tuning technique, which is called T-Few. And this is really relevant because it's much efficient. It's much cheaper, much faster. So we'll talk about what T-Few fine-tuning looks like. But traditionally, Vanilla fine-tuning basically involves updating the weights of all the layers, or most of the layers in the model.

Now, because of that, it requires longer training time, and higher serving cost. And we do support Vanilla fine-tuning in the OCI Generative AI service. But there is another technique which is supported, which is called T-Few fine-tuning. And this one, in contrast to the Vanilla fine-tuning, selectively updates only a fraction of the model's weights. And basically, this technique is an additive, what is referred to as Few-Shot Parameter Efficient Fine Tuning technique that inserts additional layers. And you can see here, the layers comprise only 0.01%, roughly 0.01% of the baseline model size. So really, really small set of layers it inserts.

And these weight updates are localized to the T-Few layers during the fine-tuning process. And because of this, the model, isolating the weight updates to only the few select layers, we are able to significantly reduce the overall training time and the cost compared to Updating all the layers with something like Vanilla fine-tuning. So there's definitely something which you can leverage, and give you really good results at a lower cost.

Now, let us look at how the T-Few fine-tuning process works. I'm not going to get into all the transformer layers, and the attention mechanism, and how the weights are propagated. But let me quickly walk you through the process itself. The process begins by utilizing the initial weights of the base model, and an annotated training data set. Now, this annotated training data set is the labeled data set, which is basically input/output pairs employed in supervised training.

So you take this annotated training data set. You have the base model weights. And then, basically, you generate a supplementary set of model weights, which is, again, roughly around 0.01% of the baseline model's size. So these fine-tune weights are generated. And now you propagate these weights to a specific group of transformer layers, which are called the T-Few transformer layers, rather than updating all the layers in the model. And doing so, basically, you reduce the training time, and also the cost.

So, again, not going into all the details. The basic idea is we start with a pre-trained model, and then fine-tune the pre-trained model using a small number of input/output examples specific to the new task or domain we are trying to work on. And this file-tuning process involves adjusting the parameters of the pre-trained model to better suit the characteristics of the new task, while still retaining the knowledge learned during the initial training phase. So in a nutshell, that's basically what the T-Few fine-tuning process looks like.

Now, one important consideration here is, using this, we can also reduce the inference cost. And inference is basically computationally expensive. So every time you're sending a request, you're getting a response back, there is cost associated with that. Now, in case of OCI Generative AI service and dedicated AI cluster, each hosting cluster-- so this is a hosting cluster-- can host one base model endpoint, and up to n fine-tuned custom model endpoints.

And each of these can serve requests concurrently. So you can see here, we have a base model. And we have several custom models which are running along with the base model. And they can serve requests concurrently. So this approach, where all these models are sharing the same GPU resource, you could also refer to as multi-tenancy, reduces the expenses associated with inference. Because inference is computationally expensive.

And endpoints, these endpoints can be deactivated to stop serving request, and reactivated later. So it's quite straightforward to do that. Now, as we do that, one question which comes up all the time is, how do we handle the GPU memory? Now, in a typical scenario, GPU memory is limited. So when you switch between models, it can incur significant overhead due to reloading the full GPU memory.

So now what do I mean by that? GPUs are particularly well suited for deep learning tasks, due to their parallel processing capabilities. But they offer a finite amount of memory. When you switch between models, the GPU typically needs to load the entire model into its memory space before it can start processing data. This is what we refer to as overhead here.

Now, this overhead includes the time and computational resources required to transfer the model data from the system memory to the GPU memory, as well as any initialization or setup tasks needed to prepare the GPU for processing with the new model.

Now, in case of OCI Generative AI service, basically what we are doing is, these models are sharing the majority of weights, in case of a T-Few fine-tuning process. And with only slight variations between them or among them. So they can be efficiently deployed on the same GPUs in a dedicated AI cluster, as we saw on the previous slide.

So you have a base model here. And then you have several custom models. And the weight difference between the base model and model A, B, C, is minimal. Because we are updating only roughly, approximately 0.01% of the base model weights.

So what this does is, because of this architecture, this results in minimal overhead when you have to switch between models derived from the same base model. Because what is happening is, you are deploying the base model alongside its fine-tuned versions. Remember, we are using the T-Few fine-tuning here. And this allows for something which is referred to as parameter sharing, where the common parts of the models are loaded into memory once, and shared across different tasks or processes.

This significantly reduces the total amount of memory required, compared to a scenario where each model with its entire set of parameters is loaded separately. So this is how we reduce the memory overhead. And as you can see here, we are getting several inference requests. And then we are returning these responses here. So it's much lower overhead because of the inherent architecture of the OCI Generative AI service and the dedicated clusters we have.

So this is pretty much it. In this lesson, we covered fine-tuning, particularly T-Few fine-tuning. And we also looked at inference. I hope you found this lesson useful. Thanks for your time.