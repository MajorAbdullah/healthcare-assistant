Welcome to this demo on OCI Generative AI Inference API.

So in the previous demo, we saw how you can interact with the OCI Generative AI chat models using the Console Management console and what we refer to as Playground. So I can come here and all the models are listed.

I can choose a particular example, which is around generating a job description and click Submit. And now the chat model is generating a job description for a senior data visualization expert. And you can see all the details here.

Now what if I don't want to use the Management Console, but rather use the same thing in a program using an SDK programmatically? So the way you do that is you click on View code and Console makes it super easy. And I can see the code to execute this in both Java and Python.

So I'll type Python here. And you can see the code is listed right here. I can copy this code. And now I can run this locally on my machine. So the way we do that is using what we refer to as Python notebooks or Jupyter Notebooks.

Jupyter Notebooks is basically a web-based interactive development environment that lets you run Python code in the browser. So the way that it works is you have a Jupyter. When you run the code in the browser, it basically sends it to a Jupyter server that is running in the background and it interfaces with whatever kernel you are running.

So right here, I'm running the default is Python 3, so that's what I'm running. And the server evaluates the code and sends the result back. Now you can install Jupyter Notebook on your local machine using a simple command like pip install Jupyter.

And you can do that or you can use something called Anaconda. Anaconda comes with various packages and tools and makes life much easier. The way you install Anaconda is listed here and you follow these steps.

And then what you end up with is an Anaconda Navigator. And inside that, you can run these Jupyter Notebooks. So I have one listed here. I have also copied and pasted the code from the console and I've added some comments. So let me walk you through what this particular code does.

So let me zoom in first. So first thing you see here is we are importing OCI. And this is basically importing the OCI Python SDK. You are enabling access to a wide range of OCI services, including generative AI.

Now next, you need to set up some authentication parameters. So first thing is compartment ID. This is where the generative AI service will run.

The second thing I'm doing here is I'm setting a config profile. So this is the name of the profile, which is specified in the config file. And this file basically stores the API keys, tenancy information, and other information details needed to authenticate our request. So that's basically the config profile.

And the third line here is basically loading the configuration data from the file, allowing the script to authenticate and interact with OCI using the SDK. So this is basically setting up the authentication. And in the next demo, we will actually walk you through how you can do this. But for now, it's already set here, so we will not-- we have changed these values. And these all look good right now.

The next one is the service endpoint. And you can see that the Inference API is running in the Frankfurt region. So that's the region where I was logged in in the console.

The next line in the code is basically the generative AI inference client. And this object is allowing the script to interact with the generative AI inference API. And you have various parameters here like service endpoint, which we specified earlier, retry is specified as none. And then there's a timeout of 240 seconds-- a response of 240 seconds and a timeout of 10 seconds. So you can read more about those parameters in our documentation.

And then the next two lines here are basically setting up chat detail class, which will hold our request what we're requesting. And then there's also something called chat_request. This is an instance of the Cohere chat request class because we're using the Cohere model here, which specifies the input for the generative AI model.

And what kind of inputs? You can see all the inputs here. So this is the message which basically is the prompt saying generate a job description for a data visualization expert with three qualifications. Max tokens is 600, temperature is set as 0, and so on and so forth. We looked into some of these parameters in the theory lesson.

And then what we do is we specify the serving mode, which is on demand. So it's not pre-warmed or anything, it's run on demand. And then we specify the model ID. This is the model which is running the Cohere chat model, which is running.

And you will see that each of these models have a different kind of an OCID. So that's how we associate that this particular model is the one we are planning to run.

And then the last couple of lines are around attaching the previously defined chat request to the chat detail object here. So you can see that here. And we also attach the compartment ID.

And then finally, we send the chat details request to the generative AI service and wait for the response, which is stored in this chat response object. And we'll print the attributes of these objects.

So the way you run these notebooks is by pressing Shift Enter. And as you do that, you will see that the sign here changes and it goes to a star, which means that this notebook is being executed. The Python code is being executed.

And right here, you can see the response which came back. And let me quickly walk you through the response. The status says 200. So the response has been successful.

We are, of course, calling the Cohere command model, so it says Cohere API. And this chat history is basically the history of all the chat messages. So this is the message provided by the user. So you can see the role as user here.

And then this message is what the model returned. And here you can see the role is chat bot. So this will have the history. Remember, these are chat models. So if you have multiple chat requests going back and forth, all that history will be stored in this area here.

And then as you scroll down, you can see finish region as complete. So this request is complete, you can see here. And then this text object is where the output is stored for this current iteration. So you can see that's what the chatbot returned.

And down here, you can see the details like model ID, model version, et cetera. So this is the way you execute the code to call the OCI generative AI service, the Inference API. And now you can run this code in a Jupyter Notebook. Or if you're running an application, you can embed this code in your application.

I hope you found this demo useful. Thanks for your time.