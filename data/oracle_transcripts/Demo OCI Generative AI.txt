
Welcome to this demo of the OCI generative AI service. Here I'm logged on to the OCI Console. And you can see that right now I am in the Germany central Frankfurt region. OCI generative AI service currently is available in a few select regions, so you should make sure wherever you are running, planning to run the service, it is available in that particular region. So it's available currently in the Frankfurt region. So I have chosen that.

To get to the service, I'm going to click on this burger menu on the left-hand side. And right under the menu, I can see a section for analytics and AI. So I click on that. And under analytics and AI, I can see AI services listed. And the first service which is listed here is generative AI. So I'll click on generative AI.

And this will bring up the generative AI dashboard. Now, you can see here a few things. There is a service tool, so you can actually click on that and watch the video if you'd like to. There is documentation, so you can read more about the service and the API endpoints. And there is something called the playground.

Playground, as it specifies here, is a visual interface for exploring the hosted, pretrained and custom models without writing a single line of code. So the idea is you can use the playground to test your use cases and refine prompts and parameters. And when you are happy with the results, you can view the code and integrate that code, generative AI code, into your applications. And actually it's quite straightforward to do that. So we'll go to the playground.

But before that, right here on the dashboard, you can also see dedicated AI clusters, which are GPU-based compute resources, which you can use for fine-tuning your models and hosting those fine-tuned models. You also have these custom models which are the fine tuned models.

And endpoints are basically the endpoints where you host your fine-tuned models for inference. So right now, you can see none of these are available because I have not done any kind of fine-tuning or I have not spun up any dedicated AI cluster.

So let's first begin by going to the playground. And right here in the playground, you can see on the left-hand side, I have two classes of pretrained foundational models. One is chat, one is embedding, as we discussed in the theory lesson. So if I click on Chat, you can see the different models available in the dropdown-- Command-R, Command-R-Plus, and Meta Llama 3, 70 billion instruct model, chat model. So those models are available.

And if I click on-- I'll actually go to embedding later on. And if I click on Model Details, you can read a little bit more about these models. So you can see some of the versions, et cetera. And if I click on this chat model link, it takes me to the OCI documentation page. And here, I can actually read more about some of these models, and I can read more about some of the documentation. I want to show you one thing here. Let me just scroll down. And I think it's right here.

So if you see these models here, you can read more about the chat models like we discussed in the theory lesson. R-Plus is a more powerful model. And you can see the prompts. The user prompt can be up to 128,000 tokens, which is actually quite a lot. Command-R is more affordable, but less powerful than Command-R-Plus.

And here, you can see that each user input can only be up to 16,000 tokens and Llama 3 model, the prompt is actually limited to just 8,000 tokens. So you can read more about these models. And you can also see which regions these models are available right now. So right now, currently when I'm recording, these chat models are available only in three OCI regions, as you can see here.

So going back to the console, let me get out of this. Here, you can chat with these models. These models, basically, the idea is the chat models keep the context of your previous prompt. And you can continue the chat with follow-up questions. So here I can say something like-- I can say something like, teach me how to fish. And if I give this particular prompt, it gives me step by step directions on how to fish and gives different steps.

But because it's a chat model, they keep the context of your previous prompt. So if I ask it to describe step 3, it basically remembers that. And step 3 is about choosing a good fishing spot. So this is where-- once it stops generating, I can actually scroll up, and I can show you. So step 3 is about choosing a location, so it gives me more detail about choosing the location.

Now, if you're happy with the output, you can click on View Code here. And you can see the code in Java and Python. And if I click on Python, I can see the code right here, I can see the inference client, the API call, all the variables I need to set up for my API to work. And literally, I could copy this code, take it to my favorite IDE, or something like a Jupyter Notebook. And I can run this, and I can actually integrate this code right into my application. So this makes life much easier because the console generates this code for us. And we can just hit Copy Code and copy that.

Now, if you're happy with this, that's great. But if you're not happy with the output, which is coming here, you can change a few things on the right-hand side. So there is something called preamble override. And this is, basically, initial guideline message that can change the model's overall chat behavior. So right here, you can see that there is a preamble, which is the default preamble. But if you want to change it, you could change something like this, which says that you are a travel advisor, answer with the pirate code.

And if you click something like Generate again, which is basically saying that I want that step 3 to be regenerated. You can now see that the output, which comes in is in a pirate tone. So you are changing the behavior of the model. But remember, we are not fine-tuning the model at this point. We are just changing the preamble, which changes the style or the tone of the model. And I can also change things like temperature, which basically decides how random the output is. So you could play around with these parameters.

Now, switching gears, if I go to the embedding models, now, embeddings basically, as we discussed in the theory lesson, are text converted to vector of numbers. And embedding makes it easier for computers to perform what is called semantic search, where you are-- where you are searching. Search function is focusing on the meaning of the text that is searching rather than finding results based on keyword, what is called as lexical search.

So here again, there are a couple of examples which are provided. I can click on Model Details. I can see the embed model and the multilingual model. And there are a couple of examples provided. So if I just click on this HR Help Center articles, basically, it lists something like 41 articles. And these are all random articles or random topics, which I don't know much about. But let's see what-- once they get converted into embeddings, what does that look like.

So if I click on Run here, it basically, gives me a visual representation. And if I click here, you can see that basically, what we have done is we have taken the text and converted them into vector of numbers. And these are nothing but vector of numbers. And we are plotting them on two dimensions. In reality, they have many more dimensions, like 384 dimensions or even larger number of dimensions. But it's difficult to visualize more than two dimensions or three dimensions. So it's doing that plot here.

But the idea is that the thing which I want to draw your attention to is if you click on some of these, your numbers are basically, the article number. Article 24 is learn about technical skills, and so on and so forth. So you can see that it took those 41 articles or article titles, and it grouped them together in different clusters. So if I click on 24, you can see this is about learning technical skills. 19 is about career development. 18 is about business skills.

So what it is doing is it's grouping articles which are semantically similar. And the way it is doing that is by putting these points together, which are numerically similar. So their vectors are somehow similar.

So that is why they are clustered here. And what that means is the embeddings which are numerically similar, which are close to each other, are also semantically similar, meaning they have similar meaning. These are all about skills. If you click here, this is about leave. This article is about vacation, this is about time card, and so on and so forth.

So you can see these in action why something like a semantic search, the way it works based on embeddings and how can-- once you have this representation, now, if a new query comes in and someone asks about skills, you would return these articles. Someone asked about time card and how to manage them or vacation or something, you would actually return them. So hopefully, that gives a quick demo on how these embeddings work. And, again, this is all available here. You can click on Code. And you can take this code, embed this in your AI applications.

Now, very quickly, let me quickly walk through some of the other characteristics. So the first one is dedicated AI clusters. These are GPU-based compute resources. So to click them is quite straightforward. Click on Create Dedicated AI Cluster, pick a name, and the cluster could be for hosting, where you are serving the inference traffic or fine-tuning-- you're fine-tuning a pretrained model.

So you could choose one of them. And then you have choice of pretrained models, and that's pretty much it. And then you click Create, and it creates your fine-tuning, your dedicated AI cluster, which can be used both for fine-tuning, as well as inference. You can also fine-tune these models. So that's basically what the custom model is.

So if you click on Create Model here, you can create your fine-tuned model. You provide a name. Click Next. And here, you can choose a base model. And you can choose a fine-tune method. And if you have not created a dedicated AI cluster, you can actually create it here. Remember, when you do fine-tuning, you're basically, you need a cluster where GPU-based compute resource so that it can run the fine-tuning job. So you could actually do it here. So this is all about creating the fine-tuning models.

And finally, once you create the fine-tuning model, you have to serve the production traffic to it, the inference traffic to it. So to do that, you create an endpoint and, again, quite straightforward, you provide a name. You provide the hosting configuration, the hosted model, and then it also requires a dedicated AI cluster.

So you could create it here. And that's how you create an endpoint. And once your endpoint is created, the model you fine-tune can serve the inference traffic using this particular endpoint. So those are other characteristics of the generative AI service.

So that's in a nutshell is a quick demo of how the Generative AI Service works. I hope you found this demo useful. Thanks for your time.