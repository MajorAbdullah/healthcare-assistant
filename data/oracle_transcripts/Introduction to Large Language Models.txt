[AUDIO LOGO]

Hi, everyone, and welcome to this OCI learning module, covering an introduction to large language models. My name is Ari. And in this module, I'll be giving a high-level overview of large language models. My goal is to make these lessons accessible to anyone, regardless of background.

However, this series will take a step technically deeper than the information you may have gathered from, say, mainstream media. All right, let's jump in. So what is a language model? The way I think about language models is that they are probabilistic models of text.

So what does that mean? Let's say we have this sentence. I wrote to the zoo to send me a pet. They sent me a-- which is the opening of a children's book. What a language model will compute for us is a distribution over a vocabulary. That means the language model knows about a set of words called a vocabulary.

And the language model is going to assign a probability to each of those words appearing in that blank. And it's important to know that when we run a sequence of words through a language model, we're going to get a back up probability for every single word in its vocabulary, but no words outside of its vocabulary.

Large language models or LLMs are no different than normal language models or simply language models. The first L in the acronym LLM does correspond to the word large, but it is somewhat meaningless. The term large has to do with the number of parameters in the model, but there is no agreed upon threshold at which a language model becomes large or extra large or not large.

In general, when people talk about LLMs, they are talking about a particular style of language model that they are using to generate text, but that need not be the case. And in fact, I've heard the term large language models used in reference to smaller models that some folks might not consider to be large, like Bert.

OK, so aside from this one example, what can LLMs do? We know that if we give an LLM a sequence of text or a prefix of text, they can compute a distribution over words in their vocabulary, but can we affect that distribution? And if so, what mechanisms do we have for changing the distribution?

That is these numbers under the words you see on the slide. And how does that affect the likelihood of these words being generated from the model? Once we have this distribution over text, how can we do things with the LLM? For example, how do we go from the distribution to generating text, which is something I'm sure we've all heard a lot about these days.

These three questions are going to be the main motivators for this learning module. In the following lessons, we're going to cover these three technical areas. First up, we'll talk about architecture. How are these models built? What do they look like under the hood?

And what do those architectures imply about what the model can do or what it's supposed to do? Next, I'll talk about how we can affect the distribution of the LLMs vocabulary. In particular, I'll talk about two ways to do this in depth. One is prompting, which does not change any of the model's parameters.

And the second is training, which does change the model's parameters. The last main topic is decoding. Decoding is the technical term for generating text from an LLM, and the way we do this is by using that vocabulary in multiple interesting ways to come up with sentences, documents, paragraphs, et cetera.

After these three topics, I'll conclude by covering one more set of work, which constitutes extensions of these three ideas and dives in to how they're being used in new and interesting ways in the academic research community, as well as the industrial research community. With that, we've now covered what LLMs are and what topics will be covered in this learning module. And we're ready to dive in to a discussion of LLM architectures.