We already have learned that OCI Generative AI offers us a variety of features to generate, summarize, and embed data. Now, let us see how does it integrate with other useful frameworks and OCI services. We can leverage all these integrations to build useful applications. Open-source frameworks offer a multitude of components to build LLM-based applications, including an integration with OCI Generative AI service. Let us begin with the integration with LangChain.

LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and rely on language model to answer based on provided context. It offers a multitude of components that help us build LLM-powered applications with minimal effort. A few components that help us build applications are large language models, prompts, memory, chains, vector stores, document loaders, text splitters, and many others. These components are easily exchangeable as well. For example, we can switch between, say, one LLM with another LLM with minimal code changes.

The core element of any language model application is the model, of course. There are two main types of models that LangChain integrates with LLMs and chat models. These are defined by their input and output types. LLM in LangChain refers to pure text completion model. They take a string prompt as input and output a string completion. Chat models are often backed by LLMs but tuned specifically for having conversations. They take a list of chat messages as input, and they return an AI message as output.

Now, let us discuss prompts. In LangChain, prompts can be created using two types of LangChain prompt classes. First is prompt template. It is created from a formatted Python string that is a combination of fixed text and any number of placeholders that are filled in when code executes. We usually use this with generation models, but we can also use it with chat models. Second is chat prompt template. It is composed of a list of chat messages, each having a role and content. We use this with chat models.

LangChain provides frameworks for creating chains of components, including LLMs and other types of components. We can compose chains in two ways. We can use language chain expression language or LCEL. It is a declarative and preferred way to create chains, or we can create chains using LangChain Python classes like LLM chain.

Now, we shall see how LangChain model prompt, and chain is used together to get response from LLM, given a user query as input. The user query can be used to invoke an LLM directly to get a response. But most of the time, in addition to the user query, we also provide more context to the LLM, like instructions or information captured at runtime. That is, when code executes. To gather this additional context, we use prompts. Prompts combine the fixed text-like instructions and variables captured at runtime to create a prompt value. And then LLM accepts prompt value as input and generate a response.

So how do we call prompt and LLM in sequence? Using chains, we can string together operations like accepting an input, passing it onto a prompt, and to get a prompt value, and passing on the prompt value to LLM to get a response.

We will use memory to store conversation with the chatbot at a point in time. Chain retrieves the conversation that is a series of chat messages from the memory using key, and passes it to the LLM along with the question. And once the chain receives a new answer to the latest query, it writes back the query and answer to the memory.

LangChain offers a variety of memory types. It all depends on what is returned from the memory. For example, we may have a chain that returns a summary of the contents of the memory compared to the actual contents of the memory. Even extracted entities like names can also be returned. Oracle 23 ai can be used as a vector store, and LangChain offers Python classes to store and search embeddings in Oracle 23 ai vector store.

OCI generative AI is used by Oracle 23 ai in a few ways. First, to generate embeddings outside the database, you can access OCI Generative AI service using DB UTILs and REST APIs. Second, Oracle 23 ai SELECT AI can use OCI Generative AI service to generate SQL to query data stored in the database using natural language. And OCI Generative AI can be used via LangChain classes. As usual, applications that use both OCI Generative AI and Oracle 23 ai vector store can be created using Python SDK as well. Thanks for watching.