[AUDIO LOGO]
Hello, welcome to the final lesson in the module covering large language models. So far, we've talked about LLM architectures, prompting, training, and decoding. We've also talked about some of the things to watch out for like memorization, prompt injection, and hallucination. In this lesson, I'm going to focus on a few applications of LLMs.
The first system that we'll cover is Retrieval Augmented Generation, otherwise known as RAG. This system is conceptually very simple. When people talk about RAG systems, typically, they're talking about a system where, first, a system user provides an input, for example, a question. Second, the system transforms that question into a query which will be used to search a database, for example, a corpus of documents.
The hope is that the search will return documents that contain the answer to the question or are otherwise relevant. Finally, the returned documents will be provided to the LLM as input in addition to the question. And the expectation is that the model will generate a correct answer. As I mentioned in the previous lesson, there is some work that shows that RAG systems, as opposed to systems that do not leverage an external corpus of documents, tend to hallucinate less. This may be intuitive.
If we give the LLM a question and then some text that contains the answer, it should be easier to answer the question by leveraging the text than answering it based solely on the documents it has seen during pre-training. RAG systems are powerful. For example, they can be used in multi-document question-answering. RAG systems are also more and more prevalent. They're used for a variety of tasks, such as dialogue, question-answering, fact-checking and others.
These systems are also elegant because they provide a non-parametric mechanism for improvement. By non-parametric, what I mean is that we don't have to touch the model at all to improve the system. All we have to do is add more documents. Let's think about this in a bit more detail.
Let's say that we have an LLM that can correctly answer a question provided that the context containing the answer is given to the model as input. Then you could take the model and answer any question you like, provided that you have a relevant corpus of documents to search and a sufficiently performant search engine. To make this more concrete, let's say that you care about answering customer questions about some software system that they are using.
In theory, in the RAG setup, all you need to do is provide the software documentation or manual as the corpus. And your LLM can now answer any question that can be answered with that manual. In practice, getting these systems to work is not trivial, because there are a few moving parts. But we've already seen a lot of RAG systems deployed in practice. And the performance of these systems seems to be improving, and the systems are ubiquitous across industry and academia. Moreover, we've seen them built on top of off-the-shelf LLMs as well as LLMs trained specifically for RAG.
Next, I'll briefly touch on LLMs for code, typically referred to as code models. As their name implies, code models are LLMs trained on code, comments, and documentation. These models have demonstrated amazing capabilities in terms of code completion and documentation completion. That is, if you provide the model with a description of the function you'd like to write, in many cases, it can just output the function for you. Some examples of these models you may have heard of include Co-pilot, Codex, and Code Llama.
Arguably, code completion might be easier than general text completion. And as such, it's easier to train performant code models. One potential reason for this is that generating code is narrower in scope than generating arbitrary text. Code is more structured. It's perhaps more repetitive and less ambiguous than natural language. Regardless of the reasons, many developers have claimed to have benefit significantly from the incorporation of code models into their workflows.
Anecdotally, these models largely eliminate the need to write any boilerplate code and commonly written functions or variables or variations of such functions. Moreover, they also shine when programming in a language that you don't know well. Instead of using a search engine to continually look up language syntax and functions, just ask the model to write the code for you.
On the flip side, more complicated tasks are still difficult or unattainable for code models. While generating code from scratch might be achievable, there is new work that shows that our best models can only automatically patch real bugs less than 15% of the time.
I want to briefly mention multi-modal models as well. These models are trained on multiple data modalities, for example text, images, and audio. These models can produce images or even video from textual descriptions and perform similar types of tasks. One interesting offshoot of this work that I'd like to make you aware of is some interesting advanced decoding techniques.
In particular, let's discuss diffustion models. If you recall, the way LLMs generate text is one word at a time. By contrast, diffusion models generate usually images all at once rather than one pixel at a time. The way they do this is by starting with an image that is simply noise-- it's not an image of anything at first-- and iteratively refining all the pixels in the image simultaneously until a coherent image emerges.
There have been some attempts at doing such a simultaneous or technically, what we would call, a joint decoding for text. But these approaches haven't achieved state-of-the-art results and are not yet popular. In fact, jointly decoding text is quite difficult, whereas, an image is a fixed size and has a fixed number of pixels. And we might know that size before beginning to generate. We typically don't know how many words we're going to generate in a sentence that we want to generate, in general. Moreover, whereas the pixels in an image take on continuous color values, words are discrete and, thus cannot be refined in a continuous manner.
Finally, I'll mention language agents. Language agents are models that are intended for sequential decision-making scenarios, for example playing chess, operating software autonomously, or browsing the web in search of an item expressed in natural language. Language agents are an extension of the classic work on machine learning agents. Yet, in this newer rendition, the systems being built also utilize LLMs.
In more detail, these models operate in what's known as an environment and iteratively take actions in pursuit of accomplishing a specific goal. For example, a language agent tasked with buying an ambiguously described product might take an action corresponding to a search. Every time the model takes an action, like searching, the environment responds, for example, with the search results.
The agent observes the results and takes another action, for example, visiting a page corresponding to a promising item. The model continues taking actions until it thinks that it has achieved its goal, at which point, it terminates. One reason that the interest in language agents is rapidly increasing is because their out-of-the-box capabilities for communication via natural language and their instruction-following capabilities. This makes it relatively easy to tell the model what it's supposed to do and what actions are available to it.
One of the canonical works in this space is known as ReAct which proposes a framework for leveraging LLMs as language agents. A key ingredient of this work is to prompt the model to emit what they call thoughts, which are summaries of the goal, what steps the model has already accomplished, and what steps the model thinks it needs to take next.
Perhaps because of the significant interest in language agents, there has been significant study of teaching LLMs how to leverage tools. Tools is used here very broadly, but boils down to using APIs and other programs to perform computation. For example, instead of doing some arithmetic by decoding, an LLM could generate some text expressing the intention to use a calculator, formulate an API call to perform the arithmetic, and then consume the result. The ability to use tools promises to greatly expand the capability of LLMs.
Finally, there is a growing body of work developing methods of training LLMs to perform various types of reasoning. LLMs that can reason successfully could be employed as high level planners in these agent systems for accomplishing highly complex, long-horizon tasks. Like humans, agents that can reason could be successful in new environments when trying to accomplish unfamiliar tasks.
This concludes the module on Large Language Models. I hope these lessons have provided an understandable while somewhat in depth technical discussion of LLMs, and I hope you learned something new. Thanks for joining.
[AUDIO LOGO]
