Welcome. In this demo, we are going to look at the embedding models. Let's head over to the playground. And in the playground, if I click on the Model dropdown, I can see embedding models listed. Now let's look at embedding models, and we'll pick the-- as you can see here, both Cohere version 3 and version 2 models are listed for English, and for multilingual, we have the version 3.0 models listed. So we'll pick the Cohere Embed English version 3.0 model. And right here, I can provide it a list of sentences.

The idea with embeddings, as you recall from the theory lesson, is embeddings are numerical representations of a piece of text converted to number sequences. With embeddings, you can compare two or more pieces of text, be it single words, sentences, paragraphs, or even longer documents, as we saw in the theory lesson. So let's go ahead, and we can add a sentence one by one, or I can just go ahead and embed these sentences from a file.

I already have a file where I've listed statements-- or rather questions about countries and their capitals, so the capital of France, Sweden, Canada, UK, et cetera. And in here, I have a statement, which is asking not about the capital of the countries, but asking about the smallest state in the United States. So let's go ahead and put these sentences through the embedding model. And this is what the output comes back. Now, what the model is doing is it's taking it's capturing the context and meaning of each piece of text, and it is representing them as embeddings, what you see here.

Each dimension of an embedding represents a certain universal characteristic about the text according to how the model understands it. Now, we can look this by compressing the embeddings to two dimensions and plotting them on a scatter plot as is done here. Now, one thing to keep in mind is as we compress the embeddings to lower dimensions, the information retained becomes less because remember in case of Cohere Embed V3 models, the model is creating 1,024 dimensional vector for each embedding, and here, we are showing two dimensions.

But we can only visualize in 2D or 3D. And it turns out that this is still a good enough approximation to help us gain intuition about the data. So this is the scatter plot, which the console gives us. And you can click here to look at the output vector projection, and it's basically saying the same thing, that these are embeddings are projected into two dimensions, plotted as points. The important thing to note here is the statement, which comes next. Vectors with more similarity will have points that are closer together.

If you recall from the theory lesson, we talked about this concept that embeddings that are numerically similar are also semantically similar, meaning text of similar meaning are indeed located close together. So you can see here, all the questions, which are about countries and their capitals, are listed here. So if I hover over there, it says, capital of South Korea, capital of Russia, et cetera.

While the sentence-- or rather the question, which is about the smallest state in the US, is actually an outlier, it's located differently than these points here. So text of similar meanings are indeed located close together. This is the idea of numerical similarity translating into semantic similarity. Now, let's go ahead and add a couple of more couple of more sentences. So I'll go here and say, what is the smallest state in India?

And if run again, you will see that the smallest state in India and the smallest state in the US are located to each other because their meanings are similar. Now, if I go and add another statement, what is the largest state in the US? You will see that it still holds true, this behavior, where the smallest states-- the questions on those are-- the embeddings are located close to each other, and they are equidistant from the question, which is around largest state in the US, and everything else is still the same. All the other capitals and the countries are located here.

So this gives you a good idea about this concept of numerical similarity and semantic similarity. Text of similar meanings are indeed located close to each other. But we also said that the model creates 1,024 dimensional vector for each embedding. So can we look at those vectors? And yes, indeed we can. So to do that, let's click on View Code. And here, the code is in Java and Python. We'll pick the Python code.

And basically, when we run this, the output is the embeddings, those floating point numbers, and you can see them. So I have already copied this code, and I've put them in a Jupyter Notebook here. So let's go ahead. And basically, this is the code which is calling the embedding model API and getting the embeddings back. So if I run this code, you will see the embeddings returned here. Now this is a blob, lots of floating point number. Remember, we have 26 embeddings in total.

And for each of the embeddings, the model is creating 1,024 dimensional vector, and these vectors are floating point numbers. So it's difficult to visualize from right here. So to do that, let me just copy this into Visual Studio Code, and then we can see these 1,024 dimensional vector. Now, before doing that, because we are looking at 26 embeddings, let's reduce that number to maybe just one because then it will be easier for us to look into that.

So let me just do that. I just have one, and let me just execute this code. And now, you will see that this particular vector, which we get back is much shorter. So let me just copy this and head over to Visual Studio Code. All right, so I've copied this in Visual Studio Code, and you can see here, this is the vector, which is the 1,024 dimensional vector. And if I want to go to a particular line here, I can do Control-- and if I go to 1,024, you can see that it's taking me right to this particular point, which is basically the endpoint for this particular embedding.

So what the embedding API is doing is, it's converting this particular question, "what is the capital of France?" So this is the sentence we have, and it's creating a 1,024 dimensional vector for this particular sentence. If I go to line 4, you can see this is the start here. So this was a quick demo where we looked at summarization and embedding models. I hope you found this demo useful. Thanks for your time.