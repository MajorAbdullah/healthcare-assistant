Welcome to this lesson on chat models available in the OCI Generative AI service. Before we dive deeper, let us look at tokens first. Large language models understand tokens rather than characters. One token can be part of a word, an entire word, or even a punctuation symbol. A common word such as "apple" is a token.

A word such as "friendship" is made up of two tokens "friend" and "ship." Number of tokens per word depend on the complexity of the text. So for a simple text, you can assume one token per word on average. For complex text, meaning text with less common words, you can assume two to three tokens per word on average.

So for example, if you have a sentence like this, many words map to one token, but some don't, indivisible, and you run this through a tokenizer for a large language model, this is an example of what a tokenizer would do, so it would break this particular sentence into multiple tokens, if you count the total number of tokens is 15, whereas the total number of words is only 10.

And in some cases, you have tokens, which are punctuations symbols, so like comma, apostrophe, period, and some words which are less frequently encountered like indivisible actually make up two tokens, so endive and Isabel. So this is an example of how a tokenizer would take a sentence and break it down into multiple tokens.

There are various pre-trained foundational chart models available in the OCI generative. AI service. So the first model, which is available is the Command-R-Plus model. This model is highly performant instruction following conversational model, and in here, the user prompt can be up to 128,000 tokens. Response can be up to 4,000 tokens for each run, and you can use it for Q&A information retrieval, sentiment analysis, et cetera, chat, et cetera.

The second model is the Command-R-16k model. This is the smaller, faster version of Command-R-Plus but it's almost as capable. And as you can see, the user prompt here can be only 16,000 tokens, not 128,000 tokens, and the response is the same up to 4,000 tokens for each particular run. You could use it interchangeably with the Command-R-Plus model, but especially, you would use it when speed and cost are important to you, so you would use Command-R-16k over Command-R-Plus

The third kind of model family, which is available is the Llama 3.1 family from Meta. Now, there are two sizes, which are available here $400 billion parameter model and 70 billion parameter model here. The user prompt and response can be up to 128,000 tokens for each run. And the 400 billion model parameter is the largest publicly available in the market today, and it's suited for complex enterprise level applications.

Now, let us look at some of the chat models you can change to get the desired output from these models. So the first parameter you can change is what is referred to as maximum output tokens, and this is the max number of tokens model generates per response. The second parameter you can change is what is referred to as preamble override. Preamble is an initial guideline message that basically can change the model's overall chat behavior and conversation style, and there is a way you can override that with the style and behavior you desire. So you could actually do that.

The third parameter you can change is what is referred to as temperature, and temperature basically controls the randomness of the output, and we'll look into each of these in more details. So starting with preamble, as we discussed earlier, preamble is nothing but an initial context or guiding message for a chat model. Now you can override this preamble. So for example, in this screenshot below you can see that the preamble override says answer in a pirate tone. The previous one actually uses, the one at the top, uses the default preamble, and you can see the output.

If you compare and contrast, you can see the output below is in a pirate tone as like a pirate is talking. So you have this option. You can change the preamble. If you don't do that, we will use the default preamble. So for example, for the Cohere Command models, this is the preamble, which is there as provided as by default. So you can definitely use it, or you can override this with something else you want to customize. The other parameter you can really use to change the output is the temperature parameter. Temperature parameter basically controls the randomness of the LLM output.

So if I give you a statement like "The sky is," the way the large language models work is they pick the next token based on probability of appearing next in this particular phrase. And as you can see here, we have different words. Blue limit, et cetera with associated probability. So this is the vocabulary of the large language model, and it picks the next token with the highest probability. So as you can guess, the next token here would be "blue." But you can control and you can change that behavior.

So if you provide a temperature of 0, you will always get blue. The model is very deterministic. You're limiting the model to use the word with the highest probability. But if you change it to something like 1, now, when the temperature increases, the distribution is flattened over all words, meaning this particular token here, the limit might actually go to 0.40, and you might actually see the output as "The sky is the limit" and so on and so forth.

The idea here is with increased temperature model uses words with lower probability. So you might see red also appear here if you change it to a higher value. The next set of parameters you can change are around top p, top k frequency and presence penalties. I have a slide describing each of these in greater detail. So let's go to those slides here. So the first parameter is around top k, and top k basically tells the model to pick the next token from the top k tokens in the list sorted by probability. So let us look at an example.

If we have a sentence like "The name of that country is the," and then we have different words here, "United," "Netherlands," "Czech," et cetera, you can see they have associated probabilities as we saw earlier. So if you set your top k as 3, model will only pick from the top 3 options and ignore everything else. So as you can see it from this example, it will pick "United," "Netherlands," and "Czech" because you have set k to top 3, and it will ignore everything else.

Most likely, it will pick United every time, but if you also change temperature, it might pick Netherlands and Czech at times. Top p is similar to top k. But now, you pick the top tokens based on the sum of their probabilities. So if we take the same statement, and now, we set top p as 0.15 or 15%, basically, it will pick from the tokens, the two tokens whose probability is sum of their probabilities will add up to 15%. So as you can see from this example, it will add it will pick United and Netherlands because if you add their probabilities, it will come out to 14.7, and it will ignore everything else, including Czech as we saw earlier.

And if you set p to 0.75, for example, the bottom 25% of the probable outputs are excluded. So this is how top p works, and you can use top p in conjunction with top k. And finally, we have these parameters around frequency and presence penalties, and the idea here is these are useful if you want to get rid of repetitions in your output. So you want your output to be more random and natural. So frequency penalty penalizes tokens that have already appeared in the preceding text and scales based on how many times the token has appeared.

So for example, if a token has appeared 10 times, it gets a higher penalty because it appeared more number of times, which reduces its probability of appearing next than a token that has appeared only once. So it basically penalizes based on the frequency. The presence penalty works exactly the same, but it applies the penalty regardless of frequency. As long as the token has appeared once before, it will get penalized.

So the idea here is you can change these parameters to get an output, which is more random and appears more natural with less repetitions. So that's it. We'll looked at the chat models available in the OCI Generative AI service, and then we looked at some of the parameters you can control to change the output you get from these models. I hope you found this lesson useful. Thanks for your time.