Welcome. In this demo, we are going to create a custom model. You can create custom models by fine-tuning the base pre-trained foundational model with your own custom dataset. Here, what we are going to do is to take a request coming from a human and rephrase it into the most accurate utterance that an AI virtual assistant should use.

So we are replacing the human thing with an AI virtual assistant response. Now, we'll use this particular dataset from the paper, Sound Control Natural Rephrasing in Dialog Systems. And you can read through this paper. It contains a number of columns, the dataset which is with this paper.

But to keep our examples simple, we'll need just the first, which is the human request and the last, the virtual assistants utterance columns. So this has many columns. We'll remove those other columns and just keep the human request column and the virtual assistant column because that's what we need.

But the data has to be in a particular format because the generative AI service accepts data in a particular format only. So let me actually walk you through what that format looks like. So here, we have the format, which the OCI generative AI service accepts.

And this is referred to as a JSONL file or a JSON lines file. And this is what is used for fine-tuning custom models in OCI. Now, JSONL is a file that contains a new JSON value or object on each line.

The file isn't evaluated as a whole like a regular JSON file would be, but rather than each line is treated as if it was a separate JSON file. This format lends itself well for storing a set of inputs in a JSON format. So the format which OCI requires has to have these two properties.

So you see, there's a property called prompt. And then there is a property called completion. So these are the two properties, which the file has to have. Each of these has to be on a new line. And these have to be UTF 8 encoded.

So you have to make sure that you meet those requirements, otherwise your custom model creation will fail. And what I was referring to earlier as what we are doing in this example is we have this human request here. Which is, for example, the first line says, ask my aunt if she can go to the JDRF walk with me on October 6.

And the virtual assistant would rephrase it to say, can you go to the JDRF walk with me on October 6? Something like that. And this particular file has some 2,000 plus examples like this.

So what we are going to do is take this file, this is our custom data. And now, we are going to fine-tune one of the cohere command light models with this particular custom data. So let's see that in action.

So here we are on the OCI generative console. And to create a custom model, I can click on the custom model link on the left hand side. And as I click that there, you can see there are no custom models existing in this particular compartment.

So what I'll do is I'll kick off this workflow by clicking on create model here. And here, you can see, I can create a new model or I can create a new version of the model. These default values I will-- optional values, I will just leave them blank and click to the next menu.

And here, I can choose my base model. Now, if you recall from the demo on dedicated AI cluster, this custom fine-tuning cluster which I created earlier actually has the small cohere unit. So we cannot use the command model because we have to match the base model to the cluster unit sizes.

So I will pick the cohere command light. Because if you use cohere command, it requires a different unit size, the large cohere unit size. So I'll pick the cohere command light model here.

And for the fine-tuning method, I have an option of choosing tfew or vanilla. I'll go with tfew, its parameter efficient fine-tuning method. And this is good for me because I have a smaller dataset, which is only 2,000 examples or so.

And here, I'm picking the dedicated cluster, which I created earlier for fine-tuning called custom fine-tuning. And right below this, I have a set of hyperparameters. Now, I can just go with the default and see what kind of output I'm getting the accuracy and the loss matrix.

And if those are not good, I can change them. But for now, these look good. So I'm happy with them. And I'll click next. And here, you can see that this content rephrasing JSON file appears in my training file.

Because what I've done is I have uploaded this particular file to a bucket here and provided the right IM policies. So my service can actually access this particular file. And you can see the first five lines from this file.

And the example we were looking at earlier is listed right here. And there is the prompt. And there is the completion. So this looks good. This is some 2,000 plus examples. And I'll hit submit here.

And now, what this will do is this will kick off the fine-tuning process. If you recall from the theory lesson, we gathered the training data, which came from that paper. We change it to be in a JSONL format. Then basically, we had the dedicated AI cluster, which was created in an earlier step.

And now, we kicked off the fine-tuning process. This process will take a few minutes to complete. And once the process will get completed, we will end up with a custom model.

And we can look at some of the model performance numbers, namely accuracy and loss. So we'll come back here in a few minutes. And hopefully, the model, the custom model would have been created by that time.